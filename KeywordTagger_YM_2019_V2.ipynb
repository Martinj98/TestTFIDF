{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Tagger Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you can find two methods with which you can tag the keywords of a certain text. In this case the texts we use are the articles pulled from the poliflw api. The two methods that we used are total word count, and term frequency - inversed document frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section belows imports all the necessary libraries. Some are quite standard such as numpy / pandas / json.\n",
    "The nltk library is very useful for natural language processing (text mining). It has features such as stop words, stemmers and tokenizers. On Github we found a 'part of speech' tagger for Dutch texts that trained on 8 million corpora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('Dutch'))\n",
    "extra_stop_words = ['waar', 'onze', 'weer', 'daarom']\n",
    "stop_words.update(extra_stop_words)\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"dutch\")\n",
    "\n",
    "# the section below imports a trained Dutch word-tagger we found on Github\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "import os\n",
    "os.chdir(r\"C:\\Users\\basje\\Documents\\Young Mavericks - Intern\\Hackathon HvU\\WordTaggerDutch\")\n",
    "tagger = PerceptronTagger(load=False)\n",
    "tagger.load('model.perc.dutch_tagger_large.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for accessing the Poliflw API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two functions in this section will help you download all the political articles through the API of poliflw and will dump the downloaded data as a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FIND_ALL_DOCUMENTS_POLIFLOW():\n",
    "    \"\"\"\n",
    "    This function searches for all possible articles through the API of Poliflow.\n",
    "    \"\"\"\n",
    "    BASE_URL = 'https://api.poliflw.nl/v0/search?scroll=1m'\n",
    "\n",
    "    scroll_id = ''    \n",
    "    total_results, total_size, size = 0, 0, 100\n",
    "     \n",
    "    all_data = []\n",
    "    while not total_size or total_results < total_size:\n",
    "        if scroll_id:\n",
    "            result = requests.get(BASE_URL + '&size=' + str(size) + '&scroll_id=' + scroll_id)\n",
    "        else:\n",
    "            result = requests.get(BASE_URL + '&size=' + str(size))\n",
    "    \n",
    "        data = result.json()\n",
    "    \n",
    "        scroll_id = data['meta']['scroll']\n",
    "        total_size = data['meta']['total']\n",
    "    \n",
    "        total_results += size\n",
    "    \n",
    "        print('%s/%s' % (total_results, total_size))\n",
    "    \n",
    "        if 'item' in data:\n",
    "            all_data += data['item']\n",
    "    \n",
    "    return all_data\n",
    "    \n",
    "def SAVE_ALL_DOCUMENTS_POLIFLOW(data):\n",
    "    \"\"\"\n",
    "    This function dumps the found data in a json-file.\n",
    "    \"\"\"\n",
    "    with open('DataDump_ArticlesPoliflow.json', 'w') as OUT:\n",
    "        json.dump(data, OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for importing and refining data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions in this section allow you to transform the json file with all the articles into a pandas dataframe and the\n",
    "second function will remove all the empty articles from the dataframe and reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSONfile_toDataframe(json_file):\n",
    "    \"\"\" \n",
    "    This function converts a json-file into a pandas dataframe.\n",
    "    The most important information per article will be stored in the dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Empty lists to fill with article data\n",
    "    dates = []\n",
    "    date_granularities = []\n",
    "    descriptions = []\n",
    "    enrichments = []\n",
    "    locations = []\n",
    "    parties = []\n",
    "    politicians = []\n",
    "    sources = []\n",
    "    titles = []\n",
    "    \n",
    "    all_lists = [dates,\n",
    "                 date_granularities,\n",
    "                 descriptions,\n",
    "                 enrichments,\n",
    "                 locations, \n",
    "                 parties,\n",
    "                 politicians,\n",
    "                 sources,\n",
    "                 titles]\n",
    "    \n",
    "    all_searches = ['date',\n",
    "                    'date_granularity',\n",
    "                    'description', \n",
    "                    'enrichments',\n",
    "                    'location',\n",
    "                    'parties',\n",
    "                    'politicians',\n",
    "                    'source',\n",
    "                    'title']\n",
    "    \n",
    "    # Loop through requested articles\n",
    "    for i in range(len(json_file)):\n",
    "        for j, searchkey in enumerate(all_searches):\n",
    "            try:\n",
    "                all_lists[j].append(json_file[i][searchkey])\n",
    "            except KeyError:\n",
    "                all_lists[j].append(None)\n",
    "\n",
    "    # Save as DataFrame and return\n",
    "    DF_search = pd.DataFrame(\n",
    "        {'date':dates,\n",
    "         'date granularity': date_granularities,\n",
    "         'description': descriptions,\n",
    "         'enrichments': enrichments,\n",
    "         'location': locations,\n",
    "         'parties': parties,\n",
    "         'politicians': politicians,\n",
    "         'source': sources,\n",
    "         'title': titles\n",
    "        })\n",
    "    \n",
    "    return DF_search\n",
    "\n",
    "def RefineDataframe(dataframe):\n",
    "    \"\"\"\n",
    "    This function removes the entries from the dataframe that do not contain a description.\n",
    "    \"\"\"\n",
    "    empty_list = []\n",
    "    \n",
    "    # loop over dataframe to record the empty entries\n",
    "    for i in range(len(dataframe)):\n",
    "        if dataframe.description[i] == None:\n",
    "            empty_list.append(i)\n",
    "    \n",
    "    # remove the empty entries by index and reset the index\n",
    "    dataframe.drop(empty_list, inplace = True)\n",
    "    dataframe = dataframe.reset_index()\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>date granularity</th>\n",
       "      <th>description</th>\n",
       "      <th>enrichments</th>\n",
       "      <th>location</th>\n",
       "      <th>parties</th>\n",
       "      <th>politicians</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-07-10T09:34:43</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Verkleining gemeenteraden gaat definitief niet...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Groningen</td>\n",
       "      <td>[GroenLinks]</td>\n",
       "      <td>None</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-05-26T10:40:32</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Zaterdag 1 juni: Bijeenkomst over krimp in Gro...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Groningen</td>\n",
       "      <td>[GroenLinks]</td>\n",
       "      <td>None</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2012-05-09T21:30:32</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Wethouder Philip Broeksma opende samen met col...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Groningen</td>\n",
       "      <td>[GroenLinks]</td>\n",
       "      <td>None</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-08-27T16:16:33</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Vanavond eerste steunfractie overleg na de hee...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Groningen</td>\n",
       "      <td>[GroenLinks]</td>\n",
       "      <td>None</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2012-08-14T13:29:56</td>\n",
       "      <td>12.0</td>\n",
       "      <td>De vakantie loopt voor een heleboel mensen lan...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Groningen</td>\n",
       "      <td>[GroenLinks]</td>\n",
       "      <td>None</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2017-05-11T00:00:00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>&lt;div class=\"text\"&gt;&amp;#13;\\n\\t\\t\\t\\t\\t\\t\\t&lt;div id...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Ridderkerk</td>\n",
       "      <td>[SGP]</td>\n",
       "      <td>None</td>\n",
       "      <td>Partij nieuws</td>\n",
       "      <td>Herman Dooyeweerd en participatie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2015-06-13T00:00:00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>&lt;div class=\"text\"&gt;&amp;#13;\\n\\t\\t\\t\\t\\t\\t\\t&lt;div id...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Ermelo</td>\n",
       "      <td>[SGP]</td>\n",
       "      <td>None</td>\n",
       "      <td>Partij nieuws</td>\n",
       "      <td>SGP-Informatieavond (15 juni 2015): Doet de PA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2017-10-03T10:59:12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>&lt;p&gt;Het bestuur van D66 Renkum heeft de concept...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Renkum</td>\n",
       "      <td>[D66]</td>\n",
       "      <td>None</td>\n",
       "      <td>Partij nieuws</td>\n",
       "      <td>Nieuwe namen op kandidatenlijst D66 Renkum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2017-06-03T10:55:18</td>\n",
       "      <td>12.0</td>\n",
       "      <td>&lt;p&gt;&lt;span style=\"color: #1d2129;\"&gt;&lt;i&gt;Dinsdagavo...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Rheden</td>\n",
       "      <td>[D66]</td>\n",
       "      <td>None</td>\n",
       "      <td>Partij nieuws</td>\n",
       "      <td>Een streep door het blowverbod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2013-12-17T00:00:00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>&lt;div class=\"text\"&gt;&amp;#13;\\n\\t\\t\\t\\t\\t\\t\\t&lt;div id...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Ermelo</td>\n",
       "      <td>[SGP]</td>\n",
       "      <td>None</td>\n",
       "      <td>Partij nieuws</td>\n",
       "      <td>Ermelo en zijn agrariers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                 date  date granularity  \\\n",
       "0      0  2013-07-10T09:34:43              12.0   \n",
       "1      1  2013-05-26T10:40:32              12.0   \n",
       "2      2  2012-05-09T21:30:32              12.0   \n",
       "3      3  2013-08-27T16:16:33              12.0   \n",
       "4      4  2012-08-14T13:29:56              12.0   \n",
       "5      5  2017-05-11T00:00:00              12.0   \n",
       "6      6  2015-06-13T00:00:00              12.0   \n",
       "7      7  2017-10-03T10:59:12              12.0   \n",
       "8      8  2017-06-03T10:55:18              12.0   \n",
       "9      9  2013-12-17T00:00:00              12.0   \n",
       "\n",
       "                                         description enrichments    location  \\\n",
       "0  Verkleining gemeenteraden gaat definitief niet...          {}   Groningen   \n",
       "1  Zaterdag 1 juni: Bijeenkomst over krimp in Gro...          {}   Groningen   \n",
       "2  Wethouder Philip Broeksma opende samen met col...          {}   Groningen   \n",
       "3  Vanavond eerste steunfractie overleg na de hee...          {}   Groningen   \n",
       "4  De vakantie loopt voor een heleboel mensen lan...          {}   Groningen   \n",
       "5  <div class=\"text\">&#13;\\n\\t\\t\\t\\t\\t\\t\\t<div id...          {}  Ridderkerk   \n",
       "6  <div class=\"text\">&#13;\\n\\t\\t\\t\\t\\t\\t\\t<div id...          {}      Ermelo   \n",
       "7  <p>Het bestuur van D66 Renkum heeft de concept...          {}      Renkum   \n",
       "8  <p><span style=\"color: #1d2129;\"><i>Dinsdagavo...          {}      Rheden   \n",
       "9  <div class=\"text\">&#13;\\n\\t\\t\\t\\t\\t\\t\\t<div id...          {}      Ermelo   \n",
       "\n",
       "        parties politicians         source  \\\n",
       "0  [GroenLinks]        None       Facebook   \n",
       "1  [GroenLinks]        None       Facebook   \n",
       "2  [GroenLinks]        None       Facebook   \n",
       "3  [GroenLinks]        None       Facebook   \n",
       "4  [GroenLinks]        None       Facebook   \n",
       "5         [SGP]        None  Partij nieuws   \n",
       "6         [SGP]        None  Partij nieuws   \n",
       "7         [D66]        None  Partij nieuws   \n",
       "8         [D66]        None  Partij nieuws   \n",
       "9         [SGP]        None  Partij nieuws   \n",
       "\n",
       "                                               title  \n",
       "0                                               None  \n",
       "1                                               None  \n",
       "2                                               None  \n",
       "3                                               None  \n",
       "4                                               None  \n",
       "5                  Herman Dooyeweerd en participatie  \n",
       "6  SGP-Informatieavond (15 juni 2015): Doet de PA...  \n",
       "7         Nieuwe namen op kandidatenlijst D66 Renkum  \n",
       "8                     Een streep door het blowverbod  \n",
       "9                           Ermelo en zijn agrariers  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path to the data    \n",
    "file_path = r'C:\\Users\\basje\\Documents\\Young Mavericks - Intern\\Junior Hackathon\\Data Dump\\all_poliflw_items.json'\n",
    "\n",
    "# load the data by opening the json file\n",
    "with open(file_path, encoding='utf-8') as data:\n",
    "    \n",
    "    # convert json file to a pandas dataframe\n",
    "    DF_all = JSONfile_toDataframe(json.load(data))\n",
    "    \n",
    "    # refine the dataframe by removing entries that containt no articles\n",
    "    DF_all = RefineDataframe(DF_all)\n",
    "    \n",
    "DF_all.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions in this section allow you to clean the texts. There are different steps in the process which are all\n",
    "combined in the last function. Several steps are taken are; removing html language, removing urls, removing special\n",
    "characters and removing stop words. The texts are also tagged and tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextMining_CleanTextRegex(text, regex_statement, replacement):\n",
    "    \"\"\"\n",
    "    This function removes the html-language from the article. Using a regex statement the function \n",
    "    clears all the text that is between '<' and '>'.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(regex_statement)\n",
    "    return re.sub(pattern, replacement, text)\n",
    "\n",
    "def TextMining_TokenizeText(text):\n",
    "    \"\"\"\n",
    "    This function tokenizes the text.\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def TextMining_CleanNonNouns(text, word_tagger):\n",
    "    \"\"\"\n",
    "    This function removes all non-noun words. Using a tagger loaded with a model that was trained using 8\n",
    "    million records of dutch sentences / words. The tagger labels all the words in the article and then the\n",
    "    function will only keep the words that are labeled as nouns.\n",
    "    \"\"\"\n",
    "    tagged = word_tagger.tag(text)\n",
    "    return [t[0] for t in tagged if 'noun' in t[1]]\n",
    "\n",
    "def TextMining_CleanStopWords(tokenized_text, stop_words):\n",
    "    \"\"\"\n",
    "    This function removes all the stop words from an article. Stop words are frequently occuring words that\n",
    "    have a relativly low meaning / weight.\n",
    "    \"\"\"\n",
    "    return [word.lower() for word in tokenized_text if word.lower() not in stop_words]\n",
    "\n",
    "def TextMining_CleanSmallWords(tokenizetext, min_wordsize=2):\n",
    "    \"\"\"\n",
    "    This function removes all words that are smaller than two characters.\n",
    "    \"\"\"\n",
    "    return [word for word in tokenizetext if len(word) >= min_wordsize]\n",
    "\n",
    "def Create_Filter(options_list):\n",
    "    \"\"\"\n",
    "    This functions creates a word list with all party names.\n",
    "    \"\"\"\n",
    "    return [opt.lower() for opt in options_list]\n",
    "\n",
    "def TextMining_CleanCitiesParties(tokenizetext, party_list, city_list):\n",
    "    \"\"\"\n",
    "    This function removes all words that are party names.\n",
    "    This function removes all words that are city names and occur as a city in the dataframe.\n",
    "    \"\"\"\n",
    "    party_filter = Create_Filter(party_list)\n",
    "    city_filter = Create_Filter(city_list)\n",
    "    return [word for word in tokenizetext if word not in set(party_filter) and word not in set(city_filter)]\n",
    "\n",
    "def TextMining_StemWords(tokenized_text):\n",
    "    \"\"\"\n",
    "    This functions stems all the words in the article.\n",
    "    \"\"\"\n",
    "    return [stemmer.stem(word) for word in tokenized_text]\n",
    "\n",
    "def TextMining_ALL(texts_list, stop_words, party_list, city_list, word_tagger):\n",
    "    \"\"\"\n",
    "    This function combines all the text cleaning steps and cleans the articles.\n",
    "    \"\"\"\n",
    "    # remove html code\n",
    "    step1 = [TextMining_CleanTextRegex(text, '<.*?>', '') for text in texts_list]\n",
    "    \n",
    "    # remove newlines ('\\n') with a space\n",
    "    step2 = [TextMining_CleanTextRegex(text, '\\n', ' ') for text in step1] \n",
    "    \n",
    "    # remove urls\n",
    "    step3 = [TextMining_CleanTextRegex(text, r'http\\S+', '') for text in step2]\n",
    "    \n",
    "    # remove everything that doesn't contain letters\n",
    "    step4 = [TextMining_CleanTextRegex(text, r\"[^a-zA-Z]+\", ' ') for text in step3]\n",
    "    \n",
    "    # tokenize text with nltk function and convert all words to lowercase\n",
    "    step5 = [TextMining_TokenizeText(text) for text in step4] \n",
    "    \n",
    "    # remove all words that are not nouns\n",
    "    step6 = [TextMining_CleanNonNouns(text, word_tagger) for text in step5]\n",
    "    \n",
    "    # remove the words that are contained within the list called stopwords\n",
    "    step7 = [TextMining_CleanStopWords(text, stop_words) for text in step6] \n",
    "    \n",
    "    # removes all words that have a length of 3 or less\n",
    "    step8 = [TextMining_CleanCitiesParties(text, party_list, city_list) for text in step7]\n",
    "    \n",
    "    # removes all small words with length 3 or lower\n",
    "    step9 = [TextMining_CleanSmallWords(text, 4) for text in step8] \n",
    "    \n",
    "    return step9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for calculating the term fequency and inversed document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions in this section are used for calculating the term frequency and inversed document frequency. Using all the \n",
    "unique words and the number of texts a dictionary is created. Using this dictionary a matrix is formed in which the \n",
    "occurence of all the words per text is counted. The inversed document frequency is calculated for each word. These are then\n",
    "combined to create the tf-idf value for each word in each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF_UniqueWords(processed_texts):\n",
    "    \"\"\"\n",
    "    This function finds all the unique words from the provided articles.\n",
    "    \"\"\"\n",
    "    temp = set([word for text in processed_texts for word in text])\n",
    "    return list(temp)\n",
    "\n",
    "def TFIDF_WordDictionary(unique_word_list):\n",
    "    \"\"\"\n",
    "    This function creates a dictionary with all the unique words and gives them an index.\n",
    "    \"\"\"\n",
    "    return {key:index for index, key in enumerate(unique_word_list)}\n",
    "    \n",
    "def TFIDF_EmptyMatrix(texts_list, unique_word_list):\n",
    "    \"\"\"\n",
    "    This function creates an empty matrix filled with zeros. The axes are the number of articles in the texts\n",
    "    list and the number of unique words.\n",
    "    \"\"\"\n",
    "    return np.zeros((len(texts_list), len(unique_word_list)))\n",
    "\n",
    "def TFIDF_CountMatrix(unique_word_list, texts_list, word_dictionary):\n",
    "    \"\"\"\n",
    "    This function filles in an empty matrix with all the occurences of every word in every article.\n",
    "    \"\"\"\n",
    "    temp_matrix = TFIDF_EmptyMatrix(texts_list, unique_word_list)\n",
    "    for i in range(len(texts_list)):\n",
    "        for word in texts_list[i]:\n",
    "            temp_matrix[i, word_dictionary[word]] += 1\n",
    "            \n",
    "    return temp_matrix\n",
    "\n",
    "def TFIDF_CleanMatrix(count_matrix):\n",
    "    \"\"\"\n",
    "    This function can remove all the articles from the matrix that do not contain any words due to the text\n",
    "    cleaning operations.\n",
    "    \"\"\"\n",
    "    return count_matrix[np.where(count_matrix.sum(axis=1) != 0)]\n",
    "\n",
    "def TFIDF_TermFrequency(count_matrix):\n",
    "    \"\"\"\n",
    "    This function calculates the term frequency of each word per article. The outcome is a filled in matrix with\n",
    "    term frequency values for every word per article.\n",
    "    \"\"\"\n",
    "    sums = count_matrix.sum(axis=1)\n",
    "    new_matrix = 0.5 + 0.5 * (count_matrix / sums[np.newaxis].T)\n",
    "    return new_matrix\n",
    "\n",
    "def TDIDF_InversedDocumentFrequency(count_matrix):\n",
    "    \"\"\"\n",
    "    This function calculates the inversed document frequency of every word. The outcome is an array with all the\n",
    "    inversed document values per word.\n",
    "    \"\"\"\n",
    "    occurences = np.count_nonzero(count_matrix, axis=0)\n",
    "    new_array = np.log((len(count_matrix) / occurences))\n",
    "    return new_array\n",
    "\n",
    "def TFIDF_CalculateTFIDF(tf_matrix, idf_matrix):\n",
    "    \"\"\"\n",
    "    This function combines the word frequency matrix with the inversed document frequency array in order to \n",
    "    calculate all the term frequency-inversed document frequency values per word per article.\n",
    "    \"\"\"\n",
    "    return tf_matrix * idf_matrix\n",
    "\n",
    "def TFIDF_ALL(texts_list):\n",
    "    \"\"\"\n",
    "    This function combines all the tf-idf functions to create a process with which the tf-idf values can be \n",
    "    calculated for the provided articles.\n",
    "    \"\"\"\n",
    "    # find all the unique words in the provided texts\n",
    "    step1 = TFIDF_UniqueWords(texts_list)\n",
    "    \n",
    "    # convert the all the unique words to a dictionary with a given index\n",
    "    step2 = TFIDF_WordDictionary(step1)\n",
    "    \n",
    "    # create a matrix with all the word occurences per word per text\n",
    "    step3 = TFIDF_CountMatrix(step1, texts_list, step2)\n",
    "    \n",
    "    # in the matrix calculate the term frequency for every word per text\n",
    "    step4 = TFIDF_TermFrequency(step3)\n",
    "    \n",
    "    # calculate the inversed document frequency for each word\n",
    "    step5 = TDIDF_InversedDocumentFrequency(step3)\n",
    "    \n",
    "    # combine the word frequency and the inversed document frequency\n",
    "    step6 = TFIDF_CalculateTFIDF(step4, step5)\n",
    "    \n",
    "    return [step2, step6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for finding all the unique occurences of locations and political parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dataframe_FindUniques(dataframe, choice):\n",
    "    \"\"\"\n",
    "    Create a list with all the unique occurences depending on the given choice. At the moment you can choose between\n",
    "    'party' and 'city' to create a new list with uniques.\n",
    "    \"\"\"\n",
    "    mega_list = []\n",
    "    \n",
    "    if choice == 'party':\n",
    "        for parties in dataframe.parties:\n",
    "            for party in parties:\n",
    "                mega_list.append(party)\n",
    "        \n",
    "    elif choice == 'city':\n",
    "        for city in dataframe.location:\n",
    "            if city is None:\n",
    "                continue\n",
    "            mega_list.append(city)\n",
    "    \n",
    "    mega_list = set(mega_list)\n",
    "    \n",
    "    return list(mega_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for tagging the words and finding the top words per article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions in this selection will combine everything into a result. The texts are cleaned and calculations are performed\n",
    "based on the given choice. Then, based on the results, the top words per texts are found and returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_TFIDF_TopWords_ALL(analysis_result, number=5):\n",
    "    \"\"\"\n",
    "    Find the n number of words that score the highest tf-idf rating per article. These are then presented in a list\n",
    "    with the top words per article.\n",
    "    \"\"\"\n",
    "    temp_word_dict = {}\n",
    "    for value, key in enumerate(analysis_result[0]):\n",
    "        temp_word_dict[value] = key\n",
    "            \n",
    "    top_words_inds = []\n",
    "    for arr in analysis_result[1]:\n",
    "        top_words_inds.append(list(np.argpartition(arr, -number)[-number:]))\n",
    "    \n",
    "    top_words_per_article = []\n",
    "    for indexes in top_words_inds:\n",
    "        temp = []\n",
    "        for index in indexes:\n",
    "            temp.append(temp_word_dict[index])\n",
    "        top_words_per_article.append(temp)\n",
    "        \n",
    "    return top_words_per_article\n",
    "\n",
    "def Find_Count_TopWords_ALL(analysis_result, number=5):\n",
    "    \"\"\"\n",
    "    Find the n number of words that occur the most per article. These are then presented in a list with the top \n",
    "    words per article.\n",
    "    \"\"\"\n",
    "    top_words_per_article = []\n",
    "    for art in analysis_result:\n",
    "        temp_list = []\n",
    "        most_used_words = Counter(art).most_common(number)\n",
    "        for tup in most_used_words:\n",
    "            temp_list.append(tup[0])\n",
    "            \n",
    "        top_words_per_article.append(temp_list)\n",
    "            \n",
    "    return top_words_per_article\n",
    "\n",
    "def KeywordTagger_ALL(dataframe, stop_words, word_tagger, choice):\n",
    "    \"\"\"\n",
    "    This function combines all the neccessary functions in order to create a list with the top scoring words based \n",
    "    on your choice (tf-idf or count). The result is a list with the top words per article.\n",
    "    \"\"\"\n",
    "    start_index = 0\n",
    "    end_index = 1000\n",
    "    result_list = []\n",
    "    party_list = Dataframe_FindUniques(dataframe, 'party')\n",
    "    city_list = Dataframe_FindUniques(dataframe, 'city')\n",
    "    \n",
    "    while start_index < len(dataframe):\n",
    "        \n",
    "        # select a subset of the dataframe with the start and end index, these change (with +1000) after every loop\n",
    "        raw_articles = dataframe.description[start_index:end_index]\n",
    "        # clean and prepare all the texts in the selected subset\n",
    "        cleaned_articles = TextMining_ALL(raw_articles, stop_words, party_list, city_list, word_tagger)\n",
    "        \n",
    "        if choice == 'tfidf':\n",
    "            \n",
    "            # calculate the tf-idf of each word per text\n",
    "            analysed_articles = TFIDF_ALL(cleaned_articles)\n",
    "            \n",
    "            # find the top words based on the tf-idf\n",
    "            top_words = Find_TFIDF_TopWords_ALL(analysed_articles)\n",
    "        \n",
    "        elif choice == 'count':\n",
    "            \n",
    "            # find the top words based on the count\n",
    "            top_words = Find_Count_TopWords_ALL(cleaned_articles)\n",
    "        \n",
    "        for l in top_words:\n",
    "            result_list.append(l)\n",
    "        \n",
    "        start_index += 1000\n",
    "        end_index += 1000\n",
    "        \n",
    "        print('\\r{}/{}'.format(start_index, len(dataframe)), end = '')\n",
    "        \n",
    "#        if start_index == 10000:\n",
    "#            break\n",
    "        \n",
    "    print(\"\\nTHE END!\")\n",
    "    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basje\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610000/609240\n",
      "THE END!\n",
      "610000/609240\n",
      "THE END!\n"
     ]
    }
   ],
   "source": [
    "# find the top words with the tf-idf method \n",
    "top_words_per_article_list_tfidf = KeywordTagger_ALL(DF_all, stop_words, tagger, 'tfidf')\n",
    "\n",
    "# find the top words with the count method \n",
    "top_words_per_article_list_count = KeywordTagger_ALL(DF_all, stop_words, tagger, 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for adding a list to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddColumn_toDataframe(dataframe, column_list, column_name):\n",
    "    \"\"\"\n",
    "    This function creates a new column in the dataframe using the given list and the given column name.\n",
    "    \"\"\"\n",
    "    dataframe[column_name] = column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['aantal', 'raad', 'gemeente', 'gemeenteraden', 'personen'], ['bijeenkomst', 'dorp', 'juni', 'krimp', 'dorpen'], ['maatregelen', 'avond', 'bedrijven', 'mensen', 'wethouder'], ['vakantie', 'zonnepanelen', 'gebouwen', 'overleg', 'raadsvergadering'], ['wijk', 'verkiezingen', 'leden', 'mensen', 'folders'], ['vragen', 'praktijk', 'gemeente', 'mensen', 'participatie'], ['politiekbij', 'haken', 'instellingsterrein', 'achterkleinzoon', 'discussie'], ['jaar', 'leden', 'gemeente', 'lijst', 'kandidatenlijst'], ['voorstel', 'drugsbeleid', 'gemeente', 'alcohol', 'blowverbod'], ['mensen', 'kalveren', 'boer', 'jaar', 'gemeente']] \n",
      "\n",
      "[['gemeenteraden', 'personen', 'verkleining', 'kamer', 'plan'], ['krimp', 'dorpen', 'juni', 'bijeenkomst', 'pand'], ['wethouder', 'philip', 'broeksma', 'collega', 'herwil'], ['steunfractie', 'overleg', 'vakantie', 'raadsvergadering', 'zonnepanelen'], ['mensen', 'folders', 'vakantie', 'einde', 'verkiezingscampagne'], ['participatie', 'praktijk', 'mensen', 'voorbeeld', 'gemeente'], [], ['kandidatenlijst', 'lijst', 'leden', 'namen', 'kandidaten'], ['blowverbod', 'alcohol', 'drugsbeleid', 'amendement', 'voorstel'], ['boer', 'kalveren', 'koeien', 'stal', 'gemeente']]\n"
     ]
    }
   ],
   "source": [
    "print(top_words_per_article_list_tfidf[:10], '\\n')\n",
    "\n",
    "# add the top words per article from the tf-idf method to the dataframe as a new column\n",
    "AddColumn_toDataframe(DF_all, top_words_per_article_list_tfidf, 'KEYWORDS_TFIDF')\n",
    "\n",
    "print(top_words_per_article_list_count[:10])\n",
    "\n",
    "# add the top words per article from the count method to the dataframe as a new column\n",
    "AddColumn_toDataframe(DF_all, top_words_per_article_list_count, 'KEYWORDS_COUNT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra part for word2vec!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "import logging\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "def clustering_on_wordvecs(word_vectors, num_clusters):\n",
    "    # Initalize a k-means object and use it to extract centroids\n",
    "    kmeans_clustering = KMeans(n_clusters = num_clusters, init='k-means++')\n",
    "    idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "    \n",
    "    return kmeans_clustering.cluster_centers_, idx\n",
    "\n",
    "def get_top_words(index2word, k, centers, wordvecs):\n",
    "    tree = KDTree(wordvecs)\n",
    "    #Closest points for each Cluster center is used to query the closest 20 points to it.\n",
    "    closest_points = [tree.query(np.reshape(x, (1, -1)), k=k) for x in centers]\n",
    "    closest_words_idxs = [x[1] for x in closest_points]\n",
    "    #Word Index is queried for each position in the above array, and added to a Dictionary.\n",
    "    closest_words = {}\n",
    "    for i in range(0, len(closest_words_idxs)):\n",
    "        closest_words['Cluster #' + str(i)] = [index2word[j] for j in closest_words_idxs[i][0]]\n",
    "    \n",
    "    #A DataFrame is generated from the dictionary.\n",
    "    df = pd.DataFrame(closest_words);\n",
    "    df.index = df.index + 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "cleaned_articles = TextMining_ALL(DF_all.description, stop_words, city_list, party_list, tagger) \n",
    "\n",
    "model = gensim.models.Word2Vec(cleaned_articles, size=150, window=10, min_count=10, workers=10)\n",
    "model.train(new_texts, total_examples=len(cleaned_articles), epochs=10)\n",
    "\n",
    "search_word = 'wilders'\n",
    "model.wv.most_similar(positive=search_word, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = model.wv.syn0\n",
    "\n",
    "centers, clusters = clustering_on_wordvecs(Z, 25)\n",
    "centroid_map = dict(zip(model.wv.index2word, clusters))\n",
    "\n",
    "top_words = get_top_words(model.wv.index2word, 20, centers, Z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
